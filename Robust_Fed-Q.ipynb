{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mdpgen(S, A):\n",
        "    \"\"\"\n",
        "    Generate a random MDP with S states and A actions.\n",
        "\n",
        "    Returns:\n",
        "        P    (S, S, A) array  : transition kernels P(s' | s, a)\n",
        "        R    (S, A)     array : rewards R(s, a) in [0, 10)\n",
        "        Ppi  (S, S)     array : transition matrix under uniform policy\n",
        "        p    (S,)       array : stationary distribution of Ppi\n",
        "    \"\"\"\n",
        "    # 1) Random transition kernels, then normalize rows\n",
        "    P = np.random.rand(S, S, A)\n",
        "    for j in range(A):\n",
        "        for i in range(S):\n",
        "            P[i, :, j] /= P[i, :, j].sum()\n",
        "\n",
        "    # 2) Uniform behavior policy transition matrix\n",
        "    Ppi = P.sum(axis=2) / A\n",
        "\n",
        "    # 3) Stationary distribution: left eigenvector of Ppi for eigenvalue 1\n",
        "    #    which is the right eigenvector of Ppi.T\n",
        "    eigvals, eigvecs = np.linalg.eig(Ppi.T)\n",
        "    idx = np.argmin(np.abs(eigvals - 1.0))\n",
        "    p = np.real(eigvecs[:, idx])\n",
        "    # enforce non-negativity up to a sign\n",
        "    p *= np.sign(p)\n",
        "    p /= p.sum()\n",
        "\n",
        "    # 4) Random reward function\n",
        "    R = np.random.rand(S, A)\n",
        "\n",
        "    return P, R, Ppi, p\n"
      ],
      "metadata": {
        "id": "d_gl8LfAjKmi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S, A, gamma, T = 10, 5, 0.5, 250000\n",
        "P, R, Ppi, p = mdpgen(S, A)\n",
        "def value_iteration(P, R, gamma, T):\n",
        "    \"\"\"\n",
        "    Run value iteration for T steps to compute optimal Q-function.\n",
        "    Returns:\n",
        "        Q     (S, A)   array : estimated state-action values\n",
        "        Err   (T,)     array : infinity-norm Bellman residual at each iteration\n",
        "    \"\"\"\n",
        "    S, _, A = P.shape\n",
        "    Q = np.zeros((S, A))\n",
        "    Err = np.zeros(T)\n",
        "    for t in range(T):\n",
        "        Q_old = Q.copy()\n",
        "        V = Q_old.max(axis=1)  # state-value function\n",
        "        # Bellman optimality update vectorized\n",
        "        Q = R + gamma * np.tensordot(P, V, axes=([1], [0]))\n",
        "        Err[t] = np.max(np.abs(Q - Q_old))\n",
        "    return Q, Err\n",
        "\n"
      ],
      "metadata": {
        "id": "GomDRJHLjNER"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate MDP and optimal Q-values\n",
        "#P, R = mdpgen(S, A)\n",
        "Qopt,_ = value_iteration(P, R, gamma, T=250000)\n",
        "#Qopt = np.array(Qopt)"
      ],
      "metadata": {
        "id": "9B_nYN5kKFw5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, val in [('P', P), ('R', R), ('p', p), ('Qopt', Qopt), ('gamma', gamma), ('S', S), ('A', A)]:\n",
        "    print(f\"{name}: type={type(val)}, shape={getattr(val, 'shape', 'N/A')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbYORvrPKAOw",
        "outputId": "52f6cb98-215f-4994-ffa4-52856e8f499c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P: type=<class 'numpy.ndarray'>, shape=(10, 10, 5)\n",
            "R: type=<class 'numpy.ndarray'>, shape=(10, 5)\n",
            "p: type=<class 'numpy.ndarray'>, shape=(10,)\n",
            "Qopt: type=<class 'numpy.ndarray'>, shape=(10, 5)\n",
            "gamma: type=<class 'float'>, shape=N/A\n",
            "S: type=<class 'int'>, shape=N/A\n",
            "A: type=<class 'int'>, shape=N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('Qlearn_robust_fed_q_data_new.npz',\n",
        "         P=P,\n",
        "         R=R,\n",
        "         p=p,\n",
        "         Qopt=Qopt,\n",
        "         gamma=gamma,\n",
        "         S=np.int32(S),\n",
        "         A=np.int32(A))"
      ],
      "metadata": {
        "id": "TrIFbNgGJmNJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iibYDUh06Euj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def median_of_means(X, P_groups):\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n = len(X)\n",
        "    P = max(1, min(int(P_groups), n))\n",
        "    if P == 1:\n",
        "        return float(X.mean())\n",
        "    idx = np.random.permutation(n)\n",
        "    blocks = np.array_split(X[idx], P)\n",
        "    means = np.array([b.mean() for b in blocks if b.size > 0], dtype=float)\n",
        "    return float(np.median(means))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_mom_params(S, A, T, M, epsilon, delta):\n",
        "    bar_delta = max(delta / (S * A * T), 1e-12)\n",
        "    raw_P = int(np.ceil(8 * epsilon * M + (256.0 / 7.0) * np.log(2.0 / bar_delta)))\n",
        "    return max(1, min(raw_P, M))  # 1 ≤ P ≤ M\n",
        "\n",
        "def empirical_transition_estimates(samples, S, A, H):\n",
        "    counts = np.zeros((S, S, A))\n",
        "    for s, a, s_next in samples:\n",
        "        counts[s_next, s, a] += 1\n",
        "    # Add a small value to prevent division by zero if a state-action pair was not visited\n",
        "    P_hat = counts / (np.sum(counts, axis=0) + 1e-9)\n",
        "    return P_hat\n",
        "\n",
        "def agent_epoch_update(Q, R, P_hat, gamma):\n",
        "    S, _, A = P_hat.shape\n",
        "    d = np.zeros((S, A))\n",
        "    V = Q.max(axis=1)\n",
        "    for s in range(S):\n",
        "        for a in range(A):\n",
        "            # P_hat is (S, S, A), we need P_hat[:, s, a] which is (S,)\n",
        "            d[s, a] = R[s, a] + gamma * np.dot(P_hat[:, s, a], V)\n",
        "    return d\n",
        "\n",
        "def choose_K(M, T, gamma, c1=10):\n",
        "    return int(np.ceil(c1 * np.log(M * T) / (1 - gamma)))\n",
        "\n",
        "def compute_alpha(M, T, gamma, K):\n",
        "    return np.log(M * T) / ((1 - gamma) * K)"
      ],
      "metadata": {
        "id": "h5-IYNAVo2RQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Return (Q, residual_trace) and sample with correct indexing P_env[s, :, a] ---\n",
        "def robust_fed_q(S, A, M, T, gamma, epsilon, delta, P_env, R_env, alpha, K):\n",
        "    H = max(1, T // K)\n",
        "    Q = np.zeros((S, A))\n",
        "    residual_trace = []\n",
        "\n",
        "    for k in range(K):\n",
        "        directions = np.zeros((M, S, A))\n",
        "        for i in range(M):\n",
        "            samples = []\n",
        "            # Collect H samples for each (s,a)\n",
        "            for s in range(S):\n",
        "                for a in range(A):\n",
        "                    for _ in range(H):\n",
        "                        p_dist = P_env[s, :, a]              # P(s' | s,a)  <-- FIXED\n",
        "                        s_next = np.random.choice(S, p=p_dist)\n",
        "                        samples.append((s, a, s_next))\n",
        "\n",
        "            P_hat = empirical_transition_estimates(samples, S, A, H)\n",
        "            d = agent_epoch_update(Q, R_env, P_hat, gamma)\n",
        "\n",
        "            if np.random.rand() < epsilon:\n",
        "                d = d + np.random.laplace(scale=10.0, size=d.shape)\n",
        "\n",
        "            directions[i] = d\n",
        "\n",
        "        P_mom = choose_mom_params(S, A, T, M, epsilon, delta)\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                Q[s, a] = (1 - alpha) * Q[s, a] + alpha * median_of_means(directions[:, s, a], P_mom)\n",
        "\n",
        "        # track ∞-norm Bellman residual under TRUE env\n",
        "        V = Q.max(axis=1)\n",
        "        TQ_true = np.zeros_like(Q)\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                TQ_true[s, a] = R_env[s, a] + gamma * np.dot(P_env[s, :, a], V)  # <-- consistent with mdpgen\n",
        "        residual_trace.append(np.max(np.abs(TQ_true - Q)))\n",
        "\n",
        "    return Q, np.array(residual_trace)\n"
      ],
      "metadata": {
        "id": "kqVngLWts3_L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epsilon = 0.001\n",
        "delta = 0.05\n",
        "agent_list = [3, 10, 50, 500]\n",
        "\n",
        "traces = {}\n",
        "for M in agent_list:\n",
        "    K = choose_K(M, T, gamma)\n",
        "    alpha = compute_alpha(M, T, gamma, K)\n",
        "    Q_final, res_trace = robust_fed_q(S, A, M, T, gamma, epsilon, delta, P, R, alpha, K)\n",
        "    traces[M] = res_trace\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for M in agent_list:\n",
        "    plt.semilogy(traces[M], label=fr'$M={M}$', linewidth=4)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel(r'$\\mathbf{E_t}$', fontsize=14)\n",
        "plt.grid(True, linewidth=3, which=\"both\")\n",
        "plt.legend(title='Agents', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IsBBGgGhVVY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}